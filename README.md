# Untie-the-Knots

Untie-the-Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models
<a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https://github.com/rgtjf/Untie-the-Knots&count_bg=#E97EBA&title_bg=#555555&icon=&icon_color=#E7E7E7&title=visitors&edge_flat=false" alt="Hits"></a>

<div align="center">
    <a href="https://huggingface.co/collections/rgtjf/utk-66daf994ccff050369720281">ðŸ¤— Hugging Face</a>
    <a href="https://arxiv.org/pdf/2409.04774">ðŸ“‘ Paper</a>
</div>



## License

The content of this project itself is licensed under [LICENSE](LICENSE).


## Citation

If you find this repo helpful, please cite our paper as follows:

```
@article{tian2024utk,
  title={Untie-the-Knots: An Efficient Data Augmentation Strategy for Long-Context Pre-Training in Language Models},
  author={Junfeng Tian, Da Zheng, Yang Chen, Rui Wang, Colin Zhang, Debing Zhang},
  journal={arXiv preprint arXiv:TODO},
  year={2024}
}
```
